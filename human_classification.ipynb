{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "human_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yp6969/Object_detection_Drone/blob/main/human_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human classification convulution learning"
      ],
      "metadata": {
        "id": "yqf-RqncI6uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v1KdeIGkF-0",
        "outputId": "33505942-c157-4f32-8e5b-4ba0361e5076"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###checking if using GPU"
      ],
      "metadata": {
        "id": "5WQ8EpTgg6T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmgZx5Xhgy6M",
        "outputId": "a1acfa6b-9a73-40bd-e317-03505722e05b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jun  9 18:32:47 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Tuple\n",
        "\n",
        "import imageio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow import keras\n",
        "from keras.layers.core import (\n",
        "    Activation,\n",
        "    Reshape,\n",
        "    Flatten,\n",
        "    Dense,\n",
        ")\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D"
      ],
      "metadata": {
        "id": "yMjfEYXAJIfE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/human_detection_project/Database/human_detection_dataset"
      ],
      "metadata": {
        "id": "ufWdRWUUKH9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b88e1f-cfec-4e7f-93c6-52cdad343c84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/human_detection_project/Database/human_detection_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLxnmwOPOjOT",
        "outputId": "2ec64e7f-4bdf-44e0-bd31-d57cd8928f7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mhuman\u001b[0m/  \u001b[01;34mnon_human\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE = (256, 256, 3)"
      ],
      "metadata": {
        "id": "aB13ABzkUrAw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##load images to people and no_people lists"
      ],
      "metadata": {
        "id": "rumc43S-SMOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "people_path = \"human/\"\n",
        "no_people_path = \"non_human/\"\n",
        "people = []\n",
        "no_people = []\n",
        "range_t = [i for i in range(231)]\n",
        "\n",
        "people_dir = os.listdir(people_path)\n",
        "no_people_dir = os.listdir(no_people_path)\n",
        "\n",
        "for i in tqdm(range_t):\n",
        "    people.append(Image.open(f\"{people_path}{people_dir[i]}\"))\n",
        "\n",
        "for i in tqdm(range_t):\n",
        "    no_people.append(Image.open(f\"{no_people_path}{no_people_dir[i]}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr71xcZjOlGm",
        "outputId": "b102b59f-ac72-45d0-d403-f855a27d5f05"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231/231 [00:02<00:00, 77.28it/s] \n",
            "100%|██████████| 231/231 [00:55<00:00,  4.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_images(images):\n",
        "  \"\"\"\n",
        "  reshape the images that is not in shape: (256, 256, 3)\n",
        "  \"\"\"\n",
        "  reshaped_images = []\n",
        "  for i, img in enumerate(images):\n",
        "      if img.size != (256, 256):\n",
        "          img = img.resize((256, 256))\n",
        "      new_img = np.asarray(img)\n",
        "      new_img = new_img.astype(np.float32)\n",
        "      if new_img.shape == IMAGE_SHAPE:\n",
        "          reshaped_images.append(new_img)\n",
        "  return reshaped_images"
      ],
      "metadata": {
        "id": "sqlDjBX0Q34L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "people_images = reshape_images(people)\n",
        "no_people_images = reshape_images(no_people)"
      ],
      "metadata": {
        "id": "xB7sjrTtQX-2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_shape(img_lst: list):\n",
        "  for i, img in enumerate(img_lst):\n",
        "    assert img.shape == IMAGE_SHAPE, f\"error with image number {i}\""
      ],
      "metadata": {
        "id": "sU58HOIqTReU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "chack shape of the images"
      ],
      "metadata": {
        "id": "BigkRrUkWBcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_shape(people_images)\n",
        "check_shape(no_people_images)"
      ],
      "metadata": {
        "id": "IVoBCyETTmgm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lable the images\n",
        "1 to people\n",
        "0 to non people"
      ],
      "metadata": {
        "id": "KHv7H-tVY4R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_data = [(img, 1.0) for img in people_images]\n",
        "shuffled_data.extend([(img, 0.0) for img in no_people_images])"
      ],
      "metadata": {
        "id": "RJ0SQh-9VzwT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the images\n",
        "random.shuffle(shuffled_data)"
      ],
      "metadata": {
        "id": "pkGcBvWGZFYg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##normalize the images to rgb range [0-1]"
      ],
      "metadata": {
        "id": "NnKfOh6CZI3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm = lambda mat: ((mat[0] - mat[0].min()) / mat[0].max(), mat[1])\n",
        "\n",
        "shuffled_data = map(norm, shuffled_data)\n",
        "shuffled_data = list(shuffled_data)"
      ],
      "metadata": {
        "id": "KmNYvEmwXm5y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##define imaged and lable lists"
      ],
      "metadata": {
        "id": "9q4xnOzuawpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "labels = []\n",
        "\n",
        "for img, label in shuffled_data:\n",
        "  images.append(img)\n",
        "  labels.append(label)\n",
        "\n",
        "assert len(images) == len(labels), \"labels and images should have the same length\"\n",
        "\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "0OFLon_BZjSo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##data spliting"
      ],
      "metadata": {
        "id": "rEckYajhbPx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(images)\n",
        "split = int(n * 0.7)\n",
        "\n",
        "train_images = images[: split]\n",
        "train_labels = labels[: split]\n",
        "test_images = images[split:]\n",
        "test_labels = labels[split:]\n",
        "\n",
        "# val_images = train_images[int(0.9*len(train_images)):]\n",
        "# val_labels = train_labels[int(0.9*len(train_images)):]\n",
        "\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq72SJkla4YK",
        "outputId": "36b03167-4edb-4330-ceb7-8cb4d2b6c538"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "410"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define the Model"
      ],
      "metadata": {
        "id": "xPL5qU4ncMpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##custom model"
      ],
      "metadata": {
        "id": "jS5TfqlZ78Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_generator():\n",
        "    IMAGE_SIZE = 256\n",
        "    RGB = 3\n",
        "    NFILTERS = 8\n",
        "    CONV_PARAMS = {\"kernel_size\": 3, \"use_bias\": False, \"padding\": \"same\"}\n",
        "\n",
        "    model = Sequential()\n",
        "    # model.add(Reshape((64, IMAGE_SIZE, IMAGE_SIZE, RGB), input_shape=(IMAGE_SIZE, IMAGE_SIZE, RGB)))\n",
        "    model.add(Reshape((IMAGE_SIZE, IMAGE_SIZE, RGB)))\n",
        "\n",
        "    # adding the model layers\n",
        "    for i in [1, 2, 4, 8, 16, 32, 64]:\n",
        "        model.add(Conv2D(NFILTERS * i, **CONV_PARAMS))\n",
        "        model.add(BatchNormalization(momentum=0.1))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation=\"relu\"))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    return model"
      ],
      "metadata": {
        "id": "_BRDJF0LbvCm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainer(model: Sequential, images, labels, epochs=10):\n",
        "    BATCH_SIZE = 64\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics='accuracy')  # adam = gradient descent algorithm\n",
        "    history = model.fit(x=images, y=labels, batch_size=BATCH_SIZE, verbose=1, epochs=epochs, validation_split=0.1)\n",
        "    model.save(\"model3.hdf5\")"
      ],
      "metadata": {
        "id": "86O1WljNcUmi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##evaluate the model and return the prediction"
      ],
      "metadata": {
        "id": "tNMyT-7jdVe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, test_x: np.array, test_y: np.array) -> Tuple[np.array, float]:\n",
        "    for layer in model.layers:\n",
        "      if \"batch_normalization\" in layer.name:\n",
        "        layer.trainable = False\n",
        "        # print(\"weigths:\", len(layer.weights))\n",
        "        # print(\"trainable_weights:\", len(layer.trainable_weights))\n",
        "        # print(\"non_trainable_weights:\", len(layer.non_trainable_weights))\n",
        "    # model.summary()\n",
        "    threshhold = 0.5\n",
        "    predicted_y = []\n",
        "    for i, img in enumerate(test_x):\n",
        "        pred = model.predict(img[None])\n",
        "        pred = 1 if pred[0][0] > threshhold else 0\n",
        "        predicted_y.append(pred)\n",
        "\n",
        "    accuracy = sum([1 if predicted_y[i] == int(test_y[i]) else 0 for i in range(len(predicted_y))]) / len(predicted_y)\n",
        "    with open(\"/content/drive/MyDrive/human_detection_project/results/classification_results.txt\", \"w\") as f:\n",
        "        f.write(f\"this is my prediction {predicted_y}\\n\")\n",
        "        f.write(f\"this is True {test_y}\\n\")\n",
        "        f.write(f\"accuracy score: {accuracy}\\n\")\n",
        "    return np.array(predicted_y), accuracy"
      ],
      "metadata": {
        "id": "7_Zi_DDocaLQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(images: List[np.ndarray],\n",
        "                preds: np.ndarray,\n",
        "                labels: np.ndarray,\n",
        "                size: Tuple[int, int] = (4, 4)) -> None:\n",
        "    f, axs = plt.subplots(size[0], size[1], figsize=(4 * size[0], 4 * size[1]))\n",
        "    for img, p, label, ax in zip(images, preds, labels, axs.ravel()):\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(f\"Pred: {p}, Actual: {label}\")\n",
        "        ax.axis('off')\n",
        "    plt.show(\"prediction.png\")"
      ],
      "metadata": {
        "id": "bSK9EIAKd4K4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/human_detection_project/models\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W_FAb6RfHo-",
        "outputId": "d547e097-9ce2-4fc1-9134-a261143bba01"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/human_detection_project/models\n",
            "model1.hdf5  transfer_learning_mobileNet2.hdf5\n",
            "model2.hdf5  transfer_learning_mobileNet.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate the model"
      ],
      "metadata": {
        "id": "aYBgv2D5nzCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data sets:  train_images, train_labels, test_images, test_labels\n",
        "# 291 train images, 126 test images\n",
        "\n",
        "model = model_generator()\n",
        "history = trainer(model, train_images, train_labels, epochs=30)\n",
        "history"
      ],
      "metadata": {
        "id": "KlusK7TReJCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b5096ab-15e0-46a9-eb2b-00bbb25e14bd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "5/5 [==============================] - 2s 146ms/step - loss: 2.0039 - accuracy: 0.6163 - val_loss: 0.8391 - val_accuracy: 0.7931\n",
            "Epoch 2/30\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.8232 - accuracy: 0.6744 - val_loss: 0.4925 - val_accuracy: 0.8621\n",
            "Epoch 3/30\n",
            "5/5 [==============================] - 0s 96ms/step - loss: 0.4923 - accuracy: 0.7287 - val_loss: 0.4536 - val_accuracy: 0.8621\n",
            "Epoch 4/30\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.4855 - accuracy: 0.7481 - val_loss: 0.8893 - val_accuracy: 0.7586\n",
            "Epoch 5/30\n",
            "5/5 [==============================] - 0s 97ms/step - loss: 0.4842 - accuracy: 0.7403 - val_loss: 0.3789 - val_accuracy: 0.7931\n",
            "Epoch 6/30\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.4235 - accuracy: 0.7791 - val_loss: 0.4162 - val_accuracy: 0.8276\n",
            "Epoch 7/30\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.4187 - accuracy: 0.7597 - val_loss: 1.2255 - val_accuracy: 0.7931\n",
            "Epoch 8/30\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.4315 - accuracy: 0.7791 - val_loss: 0.3454 - val_accuracy: 0.8966\n",
            "Epoch 9/30\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.4365 - accuracy: 0.7868 - val_loss: 0.2464 - val_accuracy: 0.8966\n",
            "Epoch 10/30\n",
            "5/5 [==============================] - 0s 94ms/step - loss: 0.4357 - accuracy: 0.7597 - val_loss: 0.6215 - val_accuracy: 0.7931\n",
            "Epoch 11/30\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.4518 - accuracy: 0.7636 - val_loss: 0.4681 - val_accuracy: 0.8276\n",
            "Epoch 12/30\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.3884 - accuracy: 0.7868 - val_loss: 0.4613 - val_accuracy: 0.7931\n",
            "Epoch 13/30\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.3584 - accuracy: 0.8488 - val_loss: 0.3346 - val_accuracy: 0.7931\n",
            "Epoch 14/30\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.3465 - accuracy: 0.8527 - val_loss: 0.7317 - val_accuracy: 0.8276\n",
            "Epoch 15/30\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.2746 - accuracy: 0.8837 - val_loss: 0.3308 - val_accuracy: 0.8276\n",
            "Epoch 16/30\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.2691 - accuracy: 0.8876 - val_loss: 0.6484 - val_accuracy: 0.7931\n",
            "Epoch 17/30\n",
            "5/5 [==============================] - 0s 90ms/step - loss: 0.2164 - accuracy: 0.8837 - val_loss: 0.3168 - val_accuracy: 0.9310\n",
            "Epoch 18/30\n",
            "5/5 [==============================] - 0s 85ms/step - loss: 0.2795 - accuracy: 0.8682 - val_loss: 1.4136 - val_accuracy: 0.7586\n",
            "Epoch 19/30\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.2942 - accuracy: 0.8527 - val_loss: 0.3485 - val_accuracy: 0.8276\n",
            "Epoch 20/30\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.3497 - accuracy: 0.8295 - val_loss: 0.4261 - val_accuracy: 0.8276\n",
            "Epoch 21/30\n",
            "5/5 [==============================] - 0s 86ms/step - loss: 0.5340 - accuracy: 0.7442 - val_loss: 0.3329 - val_accuracy: 0.8966\n",
            "Epoch 22/30\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.4547 - accuracy: 0.7907 - val_loss: 0.1751 - val_accuracy: 0.9310\n",
            "Epoch 23/30\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.4051 - accuracy: 0.7829 - val_loss: 0.7207 - val_accuracy: 0.7931\n",
            "Epoch 24/30\n",
            "5/5 [==============================] - 0s 87ms/step - loss: 0.4774 - accuracy: 0.8023 - val_loss: 0.3277 - val_accuracy: 0.8276\n",
            "Epoch 25/30\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.6501 - accuracy: 0.7481 - val_loss: 0.2804 - val_accuracy: 0.8621\n",
            "Epoch 26/30\n",
            "5/5 [==============================] - 0s 89ms/step - loss: 0.3495 - accuracy: 0.8527 - val_loss: 0.5156 - val_accuracy: 0.8276\n",
            "Epoch 27/30\n",
            "5/5 [==============================] - 0s 98ms/step - loss: 0.3177 - accuracy: 0.8643 - val_loss: 0.5886 - val_accuracy: 0.7586\n",
            "Epoch 28/30\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.3151 - accuracy: 0.8682 - val_loss: 0.6662 - val_accuracy: 0.5862\n",
            "Epoch 29/30\n",
            "5/5 [==============================] - 0s 88ms/step - loss: 0.3064 - accuracy: 0.8566 - val_loss: 0.4508 - val_accuracy: 0.8966\n",
            "Epoch 30/30\n",
            "5/5 [==============================] - 0s 83ms/step - loss: 0.2879 - accuracy: 0.8682 - val_loss: 0.5290 - val_accuracy: 0.8621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction, accuracy = eval(model, test_images, test_labels)"
      ],
      "metadata": {
        "id": "5maDZri9wNb2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_t = keras.models.load_model(\"model1.hdf5\")\n",
        "prediction, accuracy = eval(model_t, test_images, test_labels)"
      ],
      "metadata": {
        "id": "p7zbFHs9gGOm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzK30yOPuO0T",
        "outputId": "95743a6e-c320-4880-858e-d6d3b95c7217"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
              "       1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "       1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
              "       1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxiNJYjiwFo3",
        "outputId": "1f63aa4c-595c-456b-ace8-1e2b34202eb2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.943089430894309"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = keras.models.load_model(\"model1.hdf5\")\n",
        "# results = model_t.evaluate(test_images, test_labels, batch_size=12)\n",
        "# print(\"test loss, test acc:\", results)\n",
        "# predicted_lable, accuracy = eval(test_images, test_labels)\n",
        "# plot_images(test_images, predicted_lable, test_labels)\n",
        "\n",
        "# pred = model_t.predict(train_images[10][None])\n",
        "# print('pred', pred)\n",
        "# print('true', train_labels[10])"
      ],
      "metadata": {
        "id": "vimF4X2ugNrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ng-pTFQgynqJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}